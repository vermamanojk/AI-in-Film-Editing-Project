{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ff3bb6-9c7f-46d5-b283-64e6f7e6df6f",
   "metadata": {},
   "source": [
    "1. Problem Definition & Objective\n",
    "\n",
    "Selected Project Track: Module E: AI in Film Editing\n",
    "Problem Statement: Professional video editing is often a time-consuming and repetitive process, particularly when it comes to manual tasks like transcribing audio for subtitles, aligning text to timestamps, and applying consistent transitions between clips.\n",
    "Real-world Relevance and Motivation: This project aims to solve these inefficiencies by developing an automated pipeline that uses Artificial Intelligence to analyze video content, suggest creative titles, and generate precisely timed subtitle overlays without human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afe7d0-5639-4fcb-9d57-1cee9d57c5fd",
   "metadata": {},
   "source": [
    "2. Data Understanding & Preparation\n",
    "\n",
    "Dataset Source: Custom Synthetic/User-Uploaded Data.\n",
    "Data Loading and Exploration: The project processes high-definition MP4 video clips provided by the user to demonstrate automated editing capabilities.\n",
    "Cleaning and Preprocessing: The system extracts raw audio streams from the uploaded raw footage to prepare for deep analysis by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02d394-0e16-4241-a534-44496de5048e",
   "metadata": {},
   "source": [
    "3. Model / System Design\n",
    "\n",
    "AI Technique Used: NLP and ASR (Automatic Speech Recognition) using the OpenAI Whisper model.\n",
    "\n",
    "Architecture/Pipeline Explanation: The system is a multi-stage AI pipeline built with a \"four-phase\" workflow:\n",
    "\n",
    "Phase 1 (Transcription): Extracts audio and generates precise timestamps for every spoken segment.\n",
    "Phase 2 (Titling): Analyzes transcript context to programmatically suggest a thematic title.\n",
    "Phase 3 (Assembly): Uses MoviePy to construct a sequential Sandwich of layers with overlapping transitions.\n",
    "Phase 4 (Composition): Layers subtitles as TextClip objects synced via a calculated offset.\n",
    "Justification of Design Choices: Using a Streamlit frontend allows for seamless user control, while MoviePy enables mathematical \"negative padding\" for smooth, professional transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013781cb-9287-45ca-8b78-412fa5821144",
   "metadata": {},
   "source": [
    "4. Core Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c15d41a-ce2e-4f05-9bb8-235136be8b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ai_film_editor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ai_film_editor.py\n",
    "import streamlit as st\n",
    "import whisper\n",
    "import os\n",
    "from moviepy import VideoFileClip, concatenate_videoclips, TextClip, CompositeVideoClip, ColorClip\n",
    "import moviepy.video.fx as vfx\n",
    "from proglog import ProgressBarLogger\n",
    "\n",
    "# --- CUSTOM PROGRESS LOGGER ---\n",
    "class StreamlitProgressLogger(ProgressBarLogger):\n",
    "    def __init__(self, streamlit_progress_bar, progress_text):\n",
    "        super().__init__()\n",
    "        self.bar = streamlit_progress_bar\n",
    "        self.text_el = progress_text\n",
    "\n",
    "    def bars_callback(self, bar, attr, value, old_value=None):\n",
    "        # Calculate current percentage\n",
    "        percentage = (value / self.bars[bar]['total'])\n",
    "        # Ensure it stays within Streamlit's 0.0 - 1.0 range\n",
    "        percentage = min(max(percentage, 0.0), 1.0)\n",
    "        self.bar.progress(percentage)\n",
    "        self.text_el.text(f\"Rendering: {int(percentage * 100)}% complete\")\n",
    "\n",
    "# --- UI CONFIGURATION ---\n",
    "st.set_page_config(page_title=\"Pro AI Film Studio\", layout=\"wide\")\n",
    "st.title(\"ðŸŽ¬ AI Film Studio: Transitions, Title and Subtitles\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"1. Transition Settings\")\n",
    "    trans_style = st.selectbox(\"Style\", [\"CrossFade\", \"Slide Left\", \"Slide Right\", \"Fade Through Black\"])\n",
    "    trans_duration = st.slider(\"Duration (s)\", 0.5, 3.0, 1.0)\n",
    "    \n",
    "    st.header(\"2. Text Settings\")\n",
    "    user_custom_title = st.text_input(\"Custom Title\")\n",
    "    closing_text = st.text_input(\"Closing Message\", \"Thank You For Watching\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\"Upload Clips\", type=[\"mp4\"], accept_multiple_files=True)\n",
    "\n",
    "# --- CORE ENGINE ---\n",
    "def create_safe_text_clip(text, dur, width, height, font_size=50, pos='center'):\n",
    "    # Fix for text cutting: smaller box_width and auto-height\n",
    "    box_width = int(width * 0.65) \n",
    "    return TextClip(\n",
    "        text=text, font_size=font_size, color='white', \n",
    "        method='caption', size=(box_width, None), text_align='center'\n",
    "    ).with_duration(dur).with_position(pos)\n",
    "\n",
    "def process_video():\n",
    "    # Initialize Progress Bar\n",
    "    prog_text = st.empty()\n",
    "    prog_bar = st.progress(0.0)\n",
    "    logger = StreamlitProgressLogger(prog_bar, prog_text)\n",
    "\n",
    "    temp_paths = []\n",
    "    for i, file in enumerate(uploaded_files):\n",
    "        path = f\"temp_{i}.mp4\"\n",
    "        with open(path, \"wb\") as f: f.write(file.read())\n",
    "        temp_paths.append(path)\n",
    "    \n",
    "    main_clips = [VideoFileClip(p) for p in temp_paths]\n",
    "    w, h = main_clips[0].size\n",
    "\n",
    "    # Pass 1: Full Transcription\n",
    "    model = whisper.load_model(\"base\")\n",
    "    full_text = \"\"\n",
    "    all_segments = []\n",
    "    current_offset = 0\n",
    "    for path in temp_paths:\n",
    "        res = model.transcribe(path)\n",
    "        full_text += \" \" + res['text']\n",
    "        for s in res['segments']:\n",
    "            s['start'] += current_offset\n",
    "            s['end'] += current_offset\n",
    "            all_segments.append(s)\n",
    "        current_offset += VideoFileClip(path).duration\n",
    "\n",
    "    final_title = user_custom_title if user_custom_title else (\"THEME: \" + \" \".join(full_text.split()[:4]).upper())\n",
    "\n",
    "    # Pass 2: Assembly\n",
    "    title_scene = CompositeVideoClip([ColorClip(size=(w, h), color=(0,0,0)).with_duration(4),\n",
    "                                     create_safe_text_clip(final_title, 4, w, h, font_size=70)])\n",
    "    \n",
    "    end_scene = CompositeVideoClip([ColorClip(size=(w, h), color=(0,0,0)).with_duration(4),\n",
    "                                   create_safe_text_clip(closing_text, 4, w, h)])\n",
    "\n",
    "    # Setup Transitions\n",
    "    effect_map = {\n",
    "        \"CrossFade\": vfx.CrossFadeIn(trans_duration),\n",
    "        \"Slide Left\": vfx.SlideIn(trans_duration, side=\"left\"),\n",
    "        \"Slide Right\": vfx.SlideIn(trans_duration, side=\"right\")\n",
    "    }\n",
    "    \n",
    "    processed_main = [main_clips[0]]\n",
    "    for c in main_clips[1:]:\n",
    "        if trans_style == \"Fade Through Black\":\n",
    "            processed_main.append(c.with_effects([vfx.FadeIn(trans_duration), vfx.FadeOut(trans_duration)]))\n",
    "        else:\n",
    "            processed_main.append(c.with_effects([effect_map[trans_style]]))\n",
    "\n",
    "    padding = -trans_duration if \"Fade Through Black\" not in trans_style else 0\n",
    "    body = concatenate_videoclips(processed_main, padding=padding, method=\"compose\")\n",
    "\n",
    "    full_movie = concatenate_videoclips([\n",
    "        title_scene, \n",
    "        body.with_effects([effect_map[trans_style]]) if \"Fade Through Black\" not in trans_style else body,\n",
    "        end_scene.with_effects([effect_map[trans_style]]) if \"Fade Through Black\" not in trans_style else end_scene\n",
    "    ], padding=padding, method=\"compose\")\n",
    "\n",
    "    # Subtitles: Padded at bottom (h - 150) to avoid cutting\n",
    "    movie_start = 4 + padding\n",
    "    subs = [create_safe_text_clip(s['text'], s['end']-s['start'], w, h, font_size=32, pos=('center', h - 150))\n",
    "            .with_start(s['start'] + movie_start) for s in all_segments]\n",
    "\n",
    "    final_prod = CompositeVideoClip([full_movie] + subs)\n",
    "    \n",
    "    # Render with custom logger\n",
    "    final_prod.write_videofile(\"final_output.mp4\", codec=\"libx264\", audio_codec=\"aac\", fps=24, logger=logger)\n",
    "    st.success(\"Rendering Complete!\")\n",
    "    return \"final_output.mp4\"\n",
    "\n",
    "if st.button(\"ðŸŽ¬ Render Final Film\") and uploaded_files:\n",
    "    out = process_video()\n",
    "    st.video(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c8a0-af3d-4ccc-8f97-0c42029e77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run ai_film_editor.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f0adf-3004-4de9-910a-63188d1e9d0e",
   "metadata": {},
   "source": [
    "5. Evaluation & Analysis\n",
    "\n",
    "Quantitative Metrics: The pipeline successfully reduced the time required for basic subtitling and transition application from hours to minutes.\n",
    "Qualitative Analysis: By utilizing method='caption' for text rendering, the system successfully prevented \"text cutting\" at screen edges.\n",
    "Performance Analysis: The integration of a custom ProgressBarLogger (Proglog) provided essential real-time feedback, addressing the transparency issues often found in automated rendering scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e25654-9307-4a48-8708-cf7562bfea64",
   "metadata": {},
   "source": [
    "6. Ethical Considerations & Responsible AI\n",
    "\n",
    "Bias and Fairness: Handled by evaluating the OpenAI Whisper model's performance across diverse accents for equitable transcription quality.\n",
    "Responsible Use of AI: All AI-generated outputs (transcripts and titles) are manually reviewed and verified to ensure accuracy and alignment with project standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250da6bb-83b3-4fc8-af8c-8dc6de25bdca",
   "metadata": {},
   "source": [
    "7. Conclusion & Future Scope\n",
    "\n",
    "Summary: Successfully implemented a functional AI-driven film editor that automates subtitle generation, cinematic transitions, and thematic titling.\n",
    "Possible Improvements:\n",
    "AI Auto-Trimming: Integrate silence detection to automatically remove filler words and pauses.\n",
    "Multilingual Expansion: Implement AI translation for global subtitle support.\n",
    "Advanced Visual Tracking: Add facial recognition for \"auto-zooming\" on active speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7d808-f8c3-49d3-b820-8a30fee69ab7",
   "metadata": {},
   "source": [
    "References & AI Usage Disclosure\n",
    "\n",
    "Whisper Model: OpenAI Whisper.\n",
    "MoviePy.\n",
    "AI Disclosure: Large Language Models (LLMs) were utilized during development to debug complex rendering issues and refine documentation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b094ae2-d742-4ff6-85b8-4fae8b6e0f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
